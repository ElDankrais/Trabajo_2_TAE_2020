---
title: "TAE - Accidentalidad en Medellín - Reporte Técnico"
author: "Jaime Andres Molina Correa <br> Valentina García Velasquez <br> Felipe Villarreal Piedrahita <br> Ricardo Penaloza Velasquez <br> Daniel Chanci Restrepo"
output:
  html_document:
    theme: cosmo
    highlight: kate
    css: format.css
    df_print: paged
    code_download: yes
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
 
library("ggplot2")                     # Load ggplot2 package
library("GGally")                      # Load GGally package
if(!require(pacman)){install.packages("pacman"); library(pacman)}
pacman::p_load("tidyverse", "knitr", "leaps")
source("functions.R", local = knitr::knit_global())
knitr::opts_chunk$set(echo = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60), tidy = T)
```
#### <span>Universidad Nacional de Colombia sede Medellín</span> {.place}

#### Resumen {.letra}
En este trabajo se abarcan los ejercicios aplicados del texto guía "An Introduction to Statistical Learning with Applications in R" de la materia técnicas de aprendizaje estadístico impartida en la Universidad Nacional de Colombia sede Medellín.

# <span class="titulo_capitulo ">Capítulo 4.7 (Classification)</span> {#cap4_7}


## <span class="titulo_ejercicio">Ejercicio 10</span> {.letra}

<span class="negrilla subrayar">This question should be anwsered using the <span class="naranja">Weekly</span> data set, which is part of the <span class="naranja">ISLR</span> package. This data is similar in nature to the <span class="naranja">Smarket</span> data from this chapter’s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. </span> 

```{r, warning=FALSE, message=FALSE}
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
```
### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Produce some numerical and graphical summaries of the <span class="naranja">Weekly</span> data. Do there appear to be any patterns?</span>

```{r, warning=FALSE, message=FALSE}
library(ISLR)
data(Weekly)
attach(Weekly)
rbind(head(Weekly,10))
summary(Weekly)
#glimpse(Weekly)
#Se crea la matriz de correlacion
Weekly.corr <- round(cor(Weekly[,-9]),2)

Weekly2 <- subset( Weekly, select = -Direction )
pairs(Weekly2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Weekly2))
#ggpairs(Weekly2) 


```

Gracias a la gráfica anterior, podemos comprobar que la única gran correlación se da entre las variables año y volúmen. En realidad, ninguna de las demás varaibles logra tener una correlación mayor o igual al 10%.

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Use the full data set to perform a logistic regression with <span class="naranja">Direction</span> as the response and the five lag variables plus <span class="naranja">Volume</span> as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?</span>
```{r}
logist.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(logist.fit)
```

Debido a lo anterior, Lag2 es la única vairable que tiene un alto valor de significancia, ya que su valor p (0.0296) es menor a 0.05 y su β2 = 0.05844. Este valor es positivo, entonces muestra que si el valor fue positivo en el mercado hace 2 semanas, es más probable que lo sea en el día actual.


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.</span>

```{r}
logist.probab <- predict(logist.fit, type = "response")
logist.test <- rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"
matriz.confusion <- table(logist.test, Direction)
matriz.confusion
mosaic(matriz.confusion, shade = T, colorize = T, 
       gp = gpar(fill = matrix(c("yellow", "red", "red", "yellow"), 2, 2)))
```

Los calculos anteriores mostraron que la precisión de predicciones correctas es del 56.1% ((54 + 557) / (54 + 557 + 48 + 430)). También se logra evidenciar la gran diferencia que se da las semanas en las que el mercado sube y baja (cuando baja, la precisión es solo de 54 / (430 + 54) = 11.2%, mientras que cuando el mercado sube 557 / (557 + 48) = 92.1%).


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Now fit the logistic regression model using a training data period from 1990 to 2008, with <span class="naranja">Lag2</span> as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</span>

```{r}
data.train = (Year < 2009)
data.0910 = Weekly[!data.train, ]

logist.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = data.train)

logist.probab = predict(logist.fit, data.0910, type = "response")
logist.test = rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"

Direction.0910 = Direction[!data.train]
table(logist.test, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(logist.test == Direction.0910)*100, "%")
```

Luego de ajustar el modelo con los datos en el intervalo de tiempo dado y con la variable solicitada, se llegó a un 62.5% de predicciones correctas, lo que ocasiona un <span class="negrilla">test error rate</span> de 37.5%. En este mismo orden de ideas, se obtiene que el procetaje de observaciones correctas para las semanas con un valor en alza es de 91,8% (56/(56+5)) mientras que las semanas con un valor de mercado a la baja es de 20,93% (9/(9+34)).

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Repeat (d) using LDA.</span>

```{r}
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
table(lda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(lda.test$class == Direction.0910)*100, "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Repeat (d) using QDA.</span>

```{r}
qda.fit = qda(Direction ~ Lag2, data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)
table(qda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(qda.test$class == Direction.0910)*100,1), "%")
```

### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Repeat (d) using KNN with K = 1.</span>

```{r}
train.X = as.matrix(Lag2[data.train])
test.X = as.matrix(Lag2[!data.train])
train.Direction = Direction[data.train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(knn.pred == Direction.0910)*100,1), "%")
```


### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla">Which of these methods appears to provide the best results on this data?</span>


Aparentemente, los métodos de Regresion Logistica (62.5%) y LDA (62.5%) tienen la mayor precisión con igual resultado, en contraste con QDA (58.7%) y KNN (50%).

### <span class="negrilla">(i)</span> {.letra}
<span class="negrilla">Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.</span>


<span class="azul_cielo negrilla"> * LDA - Lag1 & Lag2: </span>

```{r}
lda.fit = lda(Direction ~ Lag2 + Lag1, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(lda.test$class == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * LDA Interacción - Lag1 & Lag2: </span>

```{r}
lda.fit = lda(Direction ~ Lag2:Lag1, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(lda.test$class == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * QDA - Lag2 & sqrt(Lag2): </span>

```{r}
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag2)), data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)$class
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(qda.test == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * QDA - [Lag2 & sqrt(Lag2)] & [Lag1 & sqrt(Lag1)]: </span>

```{r}
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag2)) + Lag1 + sqrt(abs(Lag1)), data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)$class
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(qda.test == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * Regresión logística - Lag1 & Lag2: </span>

```{r}
logist.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial, subset = data.train)
logist.probab = predict(logist.fit, data.0910, type = "response")
logist.test = rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"
Direction.0910 = Direction[!data.train]
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(logist.test == Direction.0910)*100,3), "%")
```


Para este numeral se utilzaron las viarbles Lag1(-0.041) y Lag2(0.058) ya que son las que tiene un peso mayor en la regresión logistica hecha con anterioridad.

Después de utilizar diferentes combinaciones, se observa que la precisión es muy similar, de hecho, de los métodos utilizaron, el que fue más efectivo fue el de [Lag2 & sqrt(Lag2)] & [Lag1 & sqrt(Lag1)] mientras todos los demás se mantuvieron igual.


## <span class="titulo_ejercicio">Ejercicio 11</span> {.letra}

<span class="negrilla subrayar">In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the <span class="naranja">Auto</span> data set.</span>
```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
library(caret)
library(corrplot)
```

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Create a binary variable, <span class="naranja">mpg01</span>, that contains a 1 if <span class="naranja">mpg</span> contains a value above its median, and a 0 if <span class="naranja">mpg</span> contains a value below its median. You can compute the median using the <span class="naranja">median()</span> function. Note you may find it helpful to use the <span class="naranja">data.frame()</span> function to create a single data set containing both <span class="naranja">mpg01</span> and the other <span class="naranja">Auto</span> variables.</span>

```{r, warning=FALSE, message=FALSE}
rbind(head(Auto,10))
summary(Auto)
```

```{r, warning=FALSE, message=FALSE}
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
rbind(head(Auto,5), tail(Auto,5))
str(Auto)
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Explore the data graphically in order to investigate the association between <span class="naranja">mpg01</span> and the other features. Which of the other features seem most likely to be useful in predicting <span class="naranja">mpg01</span>? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.</span>

```{r, warning=FALSE, message=FALSE}
Auto2 <- subset( Auto, select = -name )
pairs(Auto2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Auto2))


corrplot.mixed(cor(Auto2), order="hclust", tl.col="black")
```

La variable mpg01 posee cierta correlación con todas las variables. Como era de esperarse, la correlación mas positiva la tiene con mpg (84%), pero, de manera negativa, tiene una correlación considerable con displacement (-75%), cylinders (-76%) y weight (-76%).


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Split the data into a training set and a test set.</span>

```{r, warning=FALSE, message=FALSE}
set.seed(99)
partition <- createDataPartition(y = Auto$mpg01, p = 0.7, list = FALSE, times = 1)
Auto.train = Auto[partition, ]
Auto.test = Auto[-partition, ]
mpg01.test = mpg01[-partition]
```

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r}
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
lda.pred = predict(lda.fit, Auto.test)
prop.correct = mean(lda.pred$class == mpg01.test)
```

```{r, echo=FALSE}
paste("Error LDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Perform QDA on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r, warning=FALSE, message=FALSE}
# QDA
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
qda.pred = predict(qda.fit, Auto.test)
prop.correct = mean(qda.pred$class == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error QDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Perform logistic regression on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</span>

```{r}
#REGRESION LOGISTICA
logist.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train, 
    family = binomial)
logist.probs = predict(logist.fit, Auto.test, type = "response")
logist.pred = rep(0, length(logist.probs))
logist.pred[logist.probs > 0.5] = 1
prop.correct = mean(logist.pred == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error REGRESION LOGISTICA =", round((1 - prop.correct)*100,1), "%")
```


### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Perform KNN on the training data, with several values of K, in order to predict <span class="naranja">mpg01</span>. Use only the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?</span>

```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[partition, ]
test.X = cbind(cylinders, weight, displacement, horsepower)[-partition, ]
train.mpg01 = mpg01[partition]
set.seed(13)

knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
prop.correct = mean(knn.pred == mpg01.test)

```
```{r, echo=FALSE}
paste("Error con KNN (K=1) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 5)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=5) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 25)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=25) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 50)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=50) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=100) =", round((1 - prop.correct)*100,1), "%")
```

K=5 fue el valor con el menor error e todos, equivalente a 8.6%. 


## <span class="titulo_ejercicio">Ejercicio 12</span> {.letra}

<span class="negrilla subrayar">This problem involves writing functions.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Write a function, <span class="naranja">Power()</span>, that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.</span>
*Hint: Recall that x^a raises x to the power a. Use the print() function to output the result.*

```{r, warning=FALSE, message=FALSE}
Power = function() { 2^3 }
print(Power())
```

### <span class="negrilla">(b)</span> {.letra}

<span class="negrilla">Create a new function, <span class="naranja">Power2()</span>, that allows you to pass any two numbers, x and a, and prints out the value of <span class="naranja">x^a</span>. You can do this by beginning your function with the line </span>

<span class="naranja"> > Power2=function (x,a){ </span>

<span class="negrilla">You should be able to call your function by entering, for instance, </span>

<span class="naranja"> > Power2 (3,8) </span>

<span class="negrilla">on the command line. This should output the value of 3^8, namely, 6, 561. </span>

```{r, warning=FALSE, message=FALSE}
Power2 = function(x, a) { x^a }
Power2(3, 8)
```

### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Using the <span class="naranja">Power2()</span> function that you just wrote, compute 10^3, 8^17, and 131^3.</span> 

```{r, warning=FALSE, message=FALSE}
Power2(10, 3)
```

```{r, warning=FALSE, message=FALSE}
Power2(8, 17)
```

```{r, warning=FALSE, message=FALSE}
Power2(131, 3)
```


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Now create a new function, <span class="naranja">Power3()</span>, that actually returns the result <span class="naranja">x^a</span>  as an R object, rather than simply printing it to the screen. That is, if you store the value <span class="naranja">x^a</span>  in an object called result within your function, then you can simply <span class="naranja">return()</span>  this <span class="naranja">return()</span>  result, using the following line:</span>

<span class="naranja">return (result)</span>

<span class="negrilla">The line above should be the last line in your function, before the <span class="naranja">}</span> symbol.</span>

```{r, warning=FALSE, message=FALSE}
Power3 = function(x, a) {
    result = x^a
    return(result)
    }
```


### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Now using the Power3() function, create a plot of <span class="naranja">f(x) = x^2</span>. The x-axis should display a range of integers from 1 to 10, and the y-axis should display <span class="naranja">x^2</span>. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using <span class="naranja">log=‘‘x’’, log=‘‘y’’, or log=‘‘xy’’</span> as arguments to the <span class="naranja">plot()</span> function.</span>

```{r, warning=FALSE, message=FALSE}
x = 1:10
plot(x, Power3(x, 2), log = "xy", ylab = "Log(y) = x^2", xlab = "Log(x)", 
    main = "Log(x) vs Log(x^2)")
```


### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Create a function, <span class="naranja">PlotPower()</span>, that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call.</span>

<span class="naranja"> > PlotPower (1:10 ,3)</span>

<span class="negrilla">then a plot should be created with an x-axis taking on values 1, 2, . . . , 10, and a y-axis taking on values 13, 23, . . . , 103.</span>

```{r, warning=FALSE, message=FALSE}
PlotPower = function(x, a) {
    plot(x, Power3(x, a), ylab = "X^a")
}
PlotPower(1:10, 3)
```