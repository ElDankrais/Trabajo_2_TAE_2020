---
title: "TAE - SEGUNDO TRABAJO 2020-02"
author: "Jaime Andres Molina Correa <br> Valentina García Velasquez <br> Felipe Villarreal Piedrahita <br> Ricardo Penaloza Velasquez <br> Daniel Chanci Restrepo"
output:
  html_document:
    theme: cosmo
    highlight: kate
    css: format.css
    df_print: paged
    code_download: yes
    toc: yes
    number_sections: true
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(Encoding="UTF-8")
 
library("ggplot2")                     # Load ggplot2 package
library("GGally")                      # Load GGally package
if(!require(pacman)){install.packages("pacman"); library(pacman)}
pacman::p_load("tidyverse", "knitr", "leaps")
source("functions.R", local = knitr::knit_global())
knitr::opts_chunk$set(echo = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60), tidy = T)
```
<span>Universidad Nacional de Colombia sede Medellín</span> 
  
<b>Resumen: </b>
En este trabajo se abarcan los ejercicios aplicados del texto guía "An Introduction to Statistical Learning with Applications in R" de la materia técnicas de aprendizaje estadístico impartida en la Universidad Nacional de Colombia sede Medellín.

# <span class="titulo_capitulo ">Capítulo 4.7 (Classification)</span> {#cap4_7}


## <span class="titulo_ejercicio">Ejercicio 10</span> {.letra}

<span class="negrilla subrayar">This question should be anwsered using the <span class="naranja">Weekly</span> data set, which is part of the <span class="naranja">ISLR</span> package. This data is similar in nature to the <span class="naranja">Smarket</span> data from this chapter’s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. </span> 

```{r, warning=FALSE, message=FALSE}
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
```
### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Produce some numerical and graphical summaries of the <span class="naranja">Weekly</span> data. Do there appear to be any patterns?</span>

```{r, warning=FALSE, message=FALSE}
library(ISLR)
data(Weekly)
attach(Weekly)
rbind(head(Weekly,10))
summary(Weekly)
#glimpse(Weekly)
#Se crea la matriz de correlacion
Weekly.corr <- round(cor(Weekly[,-9]),2)

Weekly2 <- subset( Weekly, select = -Direction )
pairs(Weekly2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Weekly2))
#ggpairs(Weekly2) 


```

Gracias a la gráfica anterior, podemos comprobar que la única gran correlación se da entre las variables año y volúmen. En realidad, ninguna de las demás varaibles logra tener una correlación mayor o igual al 10%.

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Use the full data set to perform a logistic regression with <span class="naranja">Direction</span> as the response and the five lag variables plus <span class="naranja">Volume</span> as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?</span>
```{r}
logist.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(logist.fit)
```

Debido a lo anterior, Lag2 es la única vairable que tiene un alto valor de significancia, ya que su valor p (0.0296) es menor a 0.05 y su β2 = 0.05844. Este valor es positivo, entonces muestra que si el valor fue positivo en el mercado hace 2 semanas, es más probable que lo sea en el día actual.


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.</span>

```{r}
logist.probab <- predict(logist.fit, type = "response")
logist.test <- rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"
matriz.confusion <- table(logist.test, Direction)
matriz.confusion
mosaic(matriz.confusion, shade = T, colorize = T, 
       gp = gpar(fill = matrix(c("yellow", "red", "red", "yellow"), 2, 2)))
```

Los calculos anteriores mostraron que la precisión de predicciones correctas es del 56.1% ((54 + 557) / (54 + 557 + 48 + 430)). También se logra evidenciar la gran diferencia que se da las semanas en las que el mercado sube y baja (cuando baja, la precisión es solo de 54 / (430 + 54) = 11.2%, mientras que cuando el mercado sube 557 / (557 + 48) = 92.1%).


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Now fit the logistic regression model using a training data period from 1990 to 2008, with <span class="naranja">Lag2</span> as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</span>

```{r}
data.train = (Year < 2009)
data.0910 = Weekly[!data.train, ]

logist.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = data.train)

logist.probab = predict(logist.fit, data.0910, type = "response")
logist.test = rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"

Direction.0910 = Direction[!data.train]
table(logist.test, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(logist.test == Direction.0910)*100, "%")
```

Luego de ajustar el modelo con los datos en el intervalo de tiempo dado y con la variable solicitada, se llegó a un 62.5% de predicciones correctas, lo que ocasiona un <span class="negrilla">test error rate</span> de 37.5%. En este mismo orden de ideas, se obtiene que el procetaje de observaciones correctas para las semanas con un valor en alza es de 91,8% (56/(56+5)) mientras que las semanas con un valor de mercado a la baja es de 20,93% (9/(9+34)).

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Repeat (d) using LDA.</span>

```{r}
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
table(lda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(lda.test$class == Direction.0910)*100, "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Repeat (d) using QDA.</span>

```{r}
qda.fit = qda(Direction ~ Lag2, data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)
table(qda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(qda.test$class == Direction.0910)*100,1), "%")
```

### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Repeat (d) using KNN with K = 1.</span>

```{r}
train.X = as.matrix(Lag2[data.train])
test.X = as.matrix(Lag2[!data.train])
train.Direction = Direction[data.train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(knn.pred == Direction.0910)*100,1), "%")
```


### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla">Which of these methods appears to provide the best results on this data?</span>


Aparentemente, los métodos de Regresion Logistica (62.5%) y LDA (62.5%) tienen la mayor precisión con igual resultado, en contraste con QDA (58.7%) y KNN (50%).

### <span class="negrilla">(i)</span> {.letra}
<span class="negrilla">Experiment with different combinations of predictors, including possible transformations and interactions, for each of the methods. Report the variables, method, and associated confusion matrix that appears to provide the best results on the held out data. Note that you should also experiment with values for K in the KNN classifier.</span>


<span class="azul_cielo negrilla"> * LDA - Lag1 & Lag2: </span>

```{r}
lda.fit = lda(Direction ~ Lag2 + Lag1, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(lda.test$class == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * LDA Interacción - Lag1 & Lag2: </span>

```{r}
lda.fit = lda(Direction ~ Lag2:Lag1, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(lda.test$class == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * QDA - Lag2 & sqrt(Lag2): </span>

```{r}
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag2)), data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)$class
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(qda.test == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * QDA - [Lag2 & sqrt(Lag2)] & [Lag1 & sqrt(Lag1)]: </span>

```{r}
qda.fit = qda(Direction ~ Lag2 + sqrt(abs(Lag2)) + Lag1 + sqrt(abs(Lag1)), data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)$class
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(qda.test == Direction.0910)*100,3), "%")
```


<span class="azul_cielo negrilla"> * Regresión logística - Lag1 & Lag2: </span>

```{r}
logist.fit = glm(Direction ~ Lag1 + Lag2, data = Weekly, family = binomial, subset = data.train)
logist.probab = predict(logist.fit, data.0910, type = "response")
logist.test = rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"
Direction.0910 = Direction[!data.train]
```

```{r, echo=FALSE}
paste("Precisión =", round(mean(logist.test == Direction.0910)*100,3), "%")
```


Para este numeral se utilzaron las viarbles Lag1(-0.041) y Lag2(0.058) ya que son las que tiene un peso mayor en la regresión logistica hecha con anterioridad.

Después de utilizar diferentes combinaciones, se observa que la precisión es muy similar, de hecho, de los métodos utilizaron, el que fue más efectivo fue el de [Lag2 & sqrt(Lag2)] & [Lag1 & sqrt(Lag1)] mientras todos los demás se mantuvieron igual.


## <span class="titulo_ejercicio">Ejercicio 11</span> {.letra}

<span class="negrilla subrayar">In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the <span class="naranja">Auto</span> data set.</span>
```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
library(caret)
library(corrplot)
```

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Create a binary variable, <span class="naranja">mpg01</span>, that contains a 1 if <span class="naranja">mpg</span> contains a value above its median, and a 0 if <span class="naranja">mpg</span> contains a value below its median. You can compute the median using the <span class="naranja">median()</span> function. Note you may find it helpful to use the <span class="naranja">data.frame()</span> function to create a single data set containing both <span class="naranja">mpg01</span> and the other <span class="naranja">Auto</span> variables.</span>

```{r, warning=FALSE, message=FALSE}
rbind(head(Auto,10))
summary(Auto)
```

```{r, warning=FALSE, message=FALSE}
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
rbind(head(Auto,5), tail(Auto,5))
str(Auto)
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Explore the data graphically in order to investigate the association between <span class="naranja">mpg01</span> and the other features. Which of the other features seem most likely to be useful in predicting <span class="naranja">mpg01</span>? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.</span>

```{r, warning=FALSE, message=FALSE}
Auto2 <- subset( Auto, select = -name )
pairs(Auto2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Auto2))


corrplot.mixed(cor(Auto2), order="hclust", tl.col="black")
```

La variable mpg01 posee cierta correlación con todas las variables. Como era de esperarse, la correlación mas positiva la tiene con mpg (84%), pero, de manera negativa, tiene una correlación considerable con displacement (-75%), cylinders (-76%) y weight (-76%).


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Split the data into a training set and a test set.</span>

```{r, warning=FALSE, message=FALSE}
set.seed(99)
partition <- createDataPartition(y = Auto$mpg01, p = 0.7, list = FALSE, times = 1)
Auto.train = Auto[partition, ]
Auto.test = Auto[-partition, ]
mpg01.test = mpg01[-partition]
```

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r}
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
lda.pred = predict(lda.fit, Auto.test)
prop.correct = mean(lda.pred$class == mpg01.test)
```

```{r, echo=FALSE}
paste("Error LDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Perform QDA on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r, warning=FALSE, message=FALSE}
# QDA
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
qda.pred = predict(qda.fit, Auto.test)
prop.correct = mean(qda.pred$class == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error QDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Perform logistic regression on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</span>

```{r}
#REGRESION LOGISTICA
logist.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train, 
    family = binomial)
logist.probs = predict(logist.fit, Auto.test, type = "response")
logist.pred = rep(0, length(logist.probs))
logist.pred[logist.probs > 0.5] = 1
prop.correct = mean(logist.pred == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error REGRESION LOGISTICA =", round((1 - prop.correct)*100,1), "%")
```


### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Perform KNN on the training data, with several values of K, in order to predict <span class="naranja">mpg01</span>. Use only the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?</span>

```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[partition, ]
test.X = cbind(cylinders, weight, displacement, horsepower)[-partition, ]
train.mpg01 = mpg01[partition]
set.seed(13)

knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
prop.correct = mean(knn.pred == mpg01.test)

```
```{r, echo=FALSE}
paste("Error con KNN (K=1) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 5)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=5) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 25)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=25) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 50)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=50) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=100) =", round((1 - prop.correct)*100,1), "%")
```

K=5 fue el valor con el menor error e todos, equivalente a 8.6%. 


## <span class="titulo_ejercicio">Ejercicio 12</span> {.letra}

<span class="negrilla subrayar">This problem involves writing functions.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Write a function, <span class="naranja">Power()</span>, that prints out the result of raising 2 to the 3rd power. In other words, your function should compute 2^3 and print out the results.</span>
*Hint: Recall that x^a raises x to the power a. Use the print() function to output the result.*

```{r, warning=FALSE, message=FALSE}
Power = function() { 2^3 }
print(Power())
```

### <span class="negrilla">(b)</span> {.letra}

<span class="negrilla">Create a new function, <span class="naranja">Power2()</span>, that allows you to pass any two numbers, x and a, and prints out the value of <span class="naranja">x^a</span>. You can do this by beginning your function with the line </span>

<span class="naranja"> > Power2=function (x,a){ </span>

<span class="negrilla">You should be able to call your function by entering, for instance, </span>

<span class="naranja"> > Power2 (3,8) </span>

<span class="negrilla">on the command line. This should output the value of 3^8, namely, 6, 561. </span>

```{r, warning=FALSE, message=FALSE}
Power2 = function(x, a) { x^a }
Power2(3, 8)
```

### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Using the <span class="naranja">Power2()</span> function that you just wrote, compute 10^3, 8^17, and 131^3.</span> 

```{r, warning=FALSE, message=FALSE}
Power2(10, 3)
```

```{r, warning=FALSE, message=FALSE}
Power2(8, 17)
```

```{r, warning=FALSE, message=FALSE}
Power2(131, 3)
```


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Now create a new function, <span class="naranja">Power3()</span>, that actually returns the result <span class="naranja">x^a</span>  as an R object, rather than simply printing it to the screen. That is, if you store the value <span class="naranja">x^a</span>  in an object called result within your function, then you can simply <span class="naranja">return()</span>  this <span class="naranja">return()</span>  result, using the following line:</span>

<span class="naranja">return (result)</span>

<span class="negrilla">The line above should be the last line in your function, before the <span class="naranja">}</span> symbol.</span>

```{r, warning=FALSE, message=FALSE}
Power3 = function(x, a) {
    result = x^a
    return(result)
    }
```


### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Now using the Power3() function, create a plot of <span class="naranja">f(x) = x^2</span>. The x-axis should display a range of integers from 1 to 10, and the y-axis should display <span class="naranja">x^2</span>. Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the x-axis, the y-axis, or both on the log-scale. You can do this by using <span class="naranja">log=‘‘x’’, log=‘‘y’’, or log=‘‘xy’’</span> as arguments to the <span class="naranja">plot()</span> function.</span>

```{r, warning=FALSE, message=FALSE}
x = 1:10
plot(x, Power3(x, 2), log = "xy", ylab = "Log(y) = x^2", xlab = "Log(x)", 
    main = "Log(x) vs Log(x^2)")
```


### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Create a function, <span class="naranja">PlotPower()</span>, that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call.</span>

<span class="naranja"> > PlotPower (1:10 ,3)</span>

<span class="negrilla">then a plot should be created with an x-axis taking on values 1, 2, . . . , 10, and a y-axis taking on values 13, 23, . . . , 103.</span>

```{r message=FALSE, warning=FALSE}
PlotPower = function(x, a) {
    plot(x, Power3(x, a), ylab = "X^a")
}
PlotPower(1:10, 3)

```


# <span class="titulo_capitulo">Capítulo 8.4 (Tree-Based Methods)</span> {#cap8_4}

## <span class="titulo_ejercicio">Ejercicio 7</span> {.letra}

<span class="negrilla subrayar">In the lab, we applied random forests to the <span class="naranja">Boston</span> data using <span class="naranja">mtry=6</span> and using <span class="naranja">ntree=25</span> and <span class="naranja">ntree=500</span>. Create a plot displaying the test error resulting from random forests on this data set for a more comprehensive range of values for <span class="naranja">mtry</span> and <span class="naranja">ntree</span>. You can model your plot after Figure 8.10. Describe the results obtained.</span>

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(MASS)
library(randomForest)
```

```{r, warning=FALSE, message=FALSE}
#summary(Boston)
#Crear los subconjuntos de entreno y de prueba
p = ncol(Boston) - 1
p.2 = trunc(p/2)
p.3 = trunc(p/3)
p.raiz = trunc(sqrt(p))
set.seed(100)
train = sample(nrow(Boston), nrow(Boston)/2)
X.train = Boston[train, -14]
X.test = Boston[-train, -14]
Y.train = Boston[train, 14]
Y.test = Boston[-train, 14]
rforest.p = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, 
    mtry = p, ntree = 500)
rforest.p2 = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, 
    mtry = p.2, ntree = 500)
rforest.p3 = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, 
    mtry = p.3, ntree = 500)
rforest.praiz = randomForest(X.train, Y.train, xtest = X.test, ytest = Y.test, 
    mtry = p.raiz, ntree = 500)
plot(1:500, rforest.p$test$mse, col = "chartreuse1", type = "l", xlab = "NÚMERO DE ARBOLES", 
    ylab = "MSE DE PRUEBA", ylim = c(10, 18))
lines(1:500, rforest.p2$test$mse, col = "turquoise1", type = "l")
lines(1:500, rforest.p3$test$mse, col = "firebrick2", type = "l")
lines(1:500, rforest.praiz$test$mse, col = "royalblue1", type = "l")
legend("bottomright", c("m=p", "m=p/2", "m=p/3", "m=raiz(p)"), col = c("chartreuse1", "turquoise1",    "firebrick2", "royalblue1"),cex = 1, lty = 1)
```

<span class="negrita_verde">Para los valores de mtry se usaron además de p=13, valores recomendables de p, como p/2=6, p/3=4 y raiz(p)=3. En principio, con un solo arbol el MSE es demasiado alto, luego con alrededor de 200 arboles el MSE se estabiliza para todos los diferentes valores de predictores, en particular para p/2, p/3 y raiz(p) el error se estabiliza alrededor de 14, siendo menor siempre para m=p/2=6. Por tanto unos buenos valores para mtry y ntree serian mtry=6 y ntree=200.
</span>

## <span class="titulo_ejercicio">Ejercicio 8</span> {.letra}

<span class="negrilla subrayar">In the lab, a classification tree was applied to the <span class="naranja">Carseats</span> data set after converting <span class="naranja">Sales</span> into a qualitative response variable. Now we will seek to predict <span class="naranja">Sales</span> using regression trees and related approaches, treating the response as a quantitative variable.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla subrayar">Split the data set into a training set and a test set.</span>

```{r, message=FALSE}
library(ISLR)
attach(Carseats)
set.seed(100)
train = sample(nrow(Carseats), nrow(Carseats)/2)
Carseats.train = Carseats[train, ]
Carseats.test = Carseats[-train, ]
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla subrayar">Fit a regression tree to the training set. Plot the tree, and interpret the results. What test MSE do you obtain?</span>

```{r, warning=FALSE, message=FALSE}
library(tree)
tree.carseats = tree(Sales ~ ., data = Carseats.train,  split = "deviance")
summary(tree.carseats)
#tree.carseats
```

```{r, warning=FALSE, message=FALSE}
plot(x = tree.carseats, type = "proportional")
text(x = tree.carseats, splits = TRUE, pretty = 0,cex = 0.5, col = "forestgreen")
```

```{r}
carseats.predict = predict(tree.carseats, newdata = Carseats.test)
mean((carseats.predict - Carseats.test$Sales)^2)
```

<span class="negrita_verde"> Analizando la representación del árbol, el predictor más influyente sobre las ventas es el lugar que ocupa el producto en los estantes de las tiendas (ShelveLoc). Este hecho queda reflejado en la primera división del árbol, que separa las posiciones buenas (nodo derecho) de las malas e intermedias (nodo izquierdo).El total de nodos terminales es de 17 y el MSE de error es de alrededor de 5.4 
</span> 

### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla subrayar">Use cross-validation in order to determine the optimal level of tree complexity. Does pruning the tree improve the test MSE?</span>

```{r}
library(ggplot2)
library(ggpubr)
cv.carseats = cv.tree(tree.carseats, FUN = prune.tree)
resultados.cv = data.frame(nro.nodos = cv.carseats$size, desviacion = cv.carseats$dev,
                            k = cv.carseats$k)
p1 = ggplot(data = resultados.cv, aes(x = nro.nodos, y = desviacion)) +
      geom_line() + geom_point() + labs(title = "Error vs tamaño del árbol") + theme_bw() 
p2 = ggplot(data = resultados.cv, aes(x = k, y = desviacion)) +
      geom_line() + geom_point() + labs(title = "Error vs parametro K") + theme_bw() 
ggarrange(p1, p2)
```

```{r}
carseats.pruning = prune.tree(tree.carseats, best = 8)
plot(x = carseats.pruning, type = "proportional")
text(x = carseats.pruning, splits = TRUE, pretty = 0, cex = 0.8, col = "forestgreen")
```

```{r}
carseats.pruning.predict = predict(carseats.pruning, Carseats.test)
round(mean((Carseats.test$Sales - carseats.pruning.predict)^2),2)
```

<span class="negrita_verde">Haciendo el analisis tanto del Error vs el numero de nodos terminales como del Error vs el valor de k, se encuentra que el  numero de nodos que minimiza el error es de 8. Efectivamente el error medio de prueba de este nuevo arbol con 8 nodos finales es de 5.33
</span> 

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla subrayar">Use the bagging approach in order to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables are most important.</span>

```{r, warning=FALSE, message=FALSE}
library(randomForest)
library(tidyverse)
carseats.bagg = randomForest(Sales ~ ., data = Carseats.train, mtry = 10, ntree = 500,importance = T)
#carseats.bagg
prediccion = predict(carseats.bagg, newdata = Carseats.test)
error = round(mean((Carseats.test$Sales - prediccion)^2),2)
paste("el Error de test (mse) del modelo obtenido por bagging es:", error)
importancia_pred <- as.data.frame(importance(carseats.bagg, scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred, var = "variable")
p1 <- ggplot(data = importancia_pred, aes(x = reorder(variable, `%IncMSE`), y = `%IncMSE`, fill = `%IncMSE`)) +
    labs(x = "variable", title = "Reducción de MSE") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
p2 <- ggplot(data = importancia_pred, aes(x = reorder(variable, IncNodePurity), y = IncNodePurity,
                                          fill = IncNodePurity)) +
    labs(x = "variable", title = "Reducción de pureza") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
ggarrange(p1, p2)
```

```{r, warning=FALSE, message=FALSE}
```
<span class="negrilla">Usando un modelo de Bagging se tiene un MSE de prueba de 3.28.  
Según las dos graficas de importancia, hay dos variables con un alta importancia sobre el modelo con respecto a las otras, estas dos variables en nivel de importancia son respectivamente **ShelveLoc** y **Price**.
</span>

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla subrayar">Use random forests to analyze this data. What test MSE do you obtain? Use the importance() function to determine which variables aremost important. Describe the effect of m, the number of variables considered at each split, on the error rate obtained.</span>

```{r}
#Random Forest variando los predictores
max.predictores = ncol(Carseats)-1
nro.predictores = rep(NA, max.predictores)
test.error =  rep(NA, max.predictores)
for(i in 1:10){
  carseats.rf = randomForest(Sales ~ ., data = Carseats.train, mtry = i, ntree = 500, importance = T)
  prediccion = predict(carseats.rf, Carseats.test)
  nro.predictores[i] = i
  test.error[i] = round(mean((Carseats.test$Sales - prediccion)^2),2)
  #error = round(mean((Carseats.test$Sales - prediccion)^2),2)
  #mensaje = paste("Error de prueba con ", i, " predictores = ", error)
  #print(mensaje)
}
result.modelos = data.frame(nro.predictores, test.error)
ggplot(data = result.modelos, aes(x = nro.predictores, y = test.error)) +
  scale_x_continuous(breaks = result.modelos$nro.predictores) +
  geom_line() + geom_point() + 
  geom_point(data = result.modelos %>% arrange(test.error) %>% head(1), color = "red") +
  labs(title = "Evolución del error de predicion de prueba vs mtry",x = "nº predictores empleados", y = "Error de prueba")+   theme_bw()
```

<span class="negrita_verde">Variando el valor del numero de predictores m desde 1 hasta 10, el error de prueba disminuye significativamente hasta m=5, de ahi en adelante no se presentan grandes disminuciones, tanto así que cada vez que se corre el algoritmo se obtiene un valor diferente de m entre 5 y 9 que minimiza el error, es decir, a veces el minimo es 5, otras es 6, otras 7, otras 8 y otras incluso es 9, esto se explica gracias a que en cada corrida el algorimto usa combinaciones de predictores diferentes.
Un buen valor para usar en nuestro modelo de Random Forest podria ser entonces m = 6.
</span> 

```{r}
carseats.r6 = randomForest(Sales ~ ., data = Carseats.train, mtry = 6, ntree = 500, importance = T)
prediccion = predict(carseats.r6, Carseats.test)
error = round(mean((Carseats.test$Sales - prediccion)^2),2)
paste("Error de prueba con 6 predictores = ", error)
importancia_pred <- as.data.frame(importance(carseats.r6, scale = TRUE))
importancia_pred <- rownames_to_column(importancia_pred, var = "variable")
p1 <- ggplot(data = importancia_pred, aes(x = reorder(variable, `%IncMSE`), y = `%IncMSE`, fill = `%IncMSE`)) +
    labs(x = "variable", title = "Reducción de MSE") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
p2 <- ggplot(data = importancia_pred, aes(x = reorder(variable, IncNodePurity), y = IncNodePurity,
                                          fill = IncNodePurity)) +
    labs(x = "variable", title = "Reducción de pureza") +
    geom_col() +
    coord_flip() +
    theme_bw() +
    theme(legend.position = "bottom")
ggarrange(p1, p2)
```

<span class="negrita_verde"> Nuevamente las variables mas importantes en nuestro modelo son **ShelveLoc** y **Price** respectivamente.
</span>

## <span class="titulo_ejercicio">Ejercicio 9</span> {.letra}

<span class="negrilla subrayar">This problem involves the <span class="naranja">OJ</span> data set which is part of the <span class="naranja">ISLR</span> package.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla subrayar">Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.</span>

```{r, warning=FALSE}
library(ISLR)
library(tree)
attach(OJ)
#str(OJ)
#summary(OJ)
#rbind(head(OJ,5),tail(OJ,5))
set.seed(1000)
train = sample(nrow(OJ), 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla subrayar">Fit a tree to the training data, with Purchase as the response and the other variables as predictors. Use the summary() function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?</span>

```{r}
arbol.oj = tree(Purchase ~ ., data = OJ.train)
summary(arbol.oj)
```

<span class="negrita_verde">Se creó un arbol de clasificación con un total de 8 nodos terminales y una tasa de error de clasificación del 16%. Siendo los predictores más importantes la lealtad de marca del consumidor al producto CH **(LoyalCH)**; el precio de venta de MM menos precio de venta de CH **(PriceDiff)** y el precio de venta para MM **(SalePriceMM)**.
</span> 

### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla subrayar">Type in the name of the tree object in order to get a detailed text output. Pick one of the terminal nodes, and interpret the information displayed.</span>

```{r}
#arbol.oj
```

<span class="negrita_verde">Analizando el nodo numero 25 vemos que allí la variable decisoria es **PriceDiff** que es la diferencia entre el precio de venta del jugo de naranja Minute Maid (MM) y el precio  de venta del jugo de naranja Citrus Hill (CH). En este nodo caen todas las observacions cuyo PriceDiff sea mayor a -0.35 que son en total 104 observaciones. El error de desviacion de todas las observaciones de este nodo es de 126.70 y la prediccion es que el cliente compró Citrus Hill (CH), siendo esta la clase mayoritaria con un total del 70% de las observaciones, mientras que el restante 30% corresponde a la clase Minute Maid (MM).
</span> 

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla subrayar">Create a plot of the tree, and interpret the results.</span>

```{r}
plot(arbol.oj, type = "proportional")
text(arbol.oj, splits = TRUE, pretty = 0, cex = 0.8, col = "springgreen4")
```

<span class="negrita_verde">Se puede ver como la variable que más peso tiene en el árbol de decisión es **LoyalCH**. Ya que esta variable es el decisor de los nodos más internos, incluso da una clara clasificación para valores extremos: Si **LoyalCH < 0.27** la clasificación es indiscutiblemente **MM** y si **LoyalCH > 0.76** entonces la clasificación es **CH**. Para valores de **LoyalCH** entre **[0.27 ; 0.76]** la decisión ya depende también de *SalePriceMM* y de *PriceDiff*.
</span> 

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla subrayar">Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?</span>

```{r}
library("caret")
predicc.oj = predict(arbol.oj, OJ.test, type = "class")
confusionMatrix(predicc.oj, OJ.test$Purchase)
```
<span class="negrita_verde">Según la matriz de confusión el porcentaje de precisión es del 81.8% por lo que la tasa del error de prueba es del 18.2%
</span>

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla subrayar">Apply the cv.tree() function to the training set in order to determine the optimal tree size.</span>

```{r}
set.seed(356)
cv.arbol.oj = cv.tree(arbol.oj, FUN = prune.misclass)
```

### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla subrayar">Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.</span>

```{r}
plot(cv.arbol.oj$size, cv.arbol.oj$dev, xlab = "Nro de nodos terminales", 
     ylab = "Error de clasificación", type = "b", pch = 19)
```

### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla subrayar">Which tree size corresponds to the lowest cross-validated classification error rate?</span>

<span class="negrita_verde">A partir del grafico se ve que el número de nodos que minimiza el error de clasificación para el árbol es de 6.
</span>

### <span class="negrilla">(i)</span> {.letra}
<span class="negrilla subrayar">Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.</span>

```{r}
arbol.podado.oj = prune.tree(arbol.oj, best = 6)
plot(arbol.podado.oj, type = "proportional")
text(arbol.podado.oj, splits = TRUE, pretty = 0, cex = 0.8, col = "springgreen4")
```


### <span class="negrilla">(j)</span> {.letra}
<span class="negrilla subrayar">Compare the training error rates between the pruned and unpruned trees. Which is higher?</span>

```{r}
summary(arbol.oj)
summary(arbol.podado.oj)
```

<span class="negrita_verde">La tasa de clasificación errónea del árbol podado es un 1% mayor que la del árbol original, la tasa del original es del 16% mientras que la del árbol podado es del 17%. El error de  desviación media también es menor en el árbol original es de 0.7486 mientras que la del árbol podado es de 0.7773.
</span> 

### <span class="negrilla">(k)</span> {.letra}
<span class="negrilla subrayar">Compare the test error rates between the pruned and unpruned trees. Which is higher?</span>

```{r}
predicc.arbol.oj = predict(arbol.oj, newdata = OJ.test, type = "class")
erroneos.sinpodar = sum(OJ.test$Purchase != predicc.arbol.oj)
round(erroneos.sinpodar/length(predicc.arbol.oj),2)
predicc.arbol.podado.oj = predict(arbol.podado.oj, newdata = OJ.test, type = "class")
erroneos.podado = sum(OJ.test$Purchase != predicc.arbol.podado.oj)
round(erroneos.podado/length(predicc.arbol.podado.oj),2)
```

<span class="negrita_verde">La tasa de error de clasificacion del árbol podado (20%) es mayor que la del árbol sin podar (18%)
</span> 

## <span class="titulo_ejercicio">Ejercicio 10</span> {.letra}

<span class="negrilla subrayar">We now use boosting to predict <span class="naranja">Salary</span> in the <span class="naranja">Hitters</span> data set.</span>

### <span class="negrilla">(a)</span> {.letra}
Remove the observations for whom the salary information is unknown, and then log-transform the salaries.

```{r, warning=FALSE, message=FALSE}
library(ISLR)
sum(is.na(Hitters$Salary))
```

```{r}
Hitters = Hitters[-which(is.na(Hitters$Salary)), ]
sum(is.na(Hitters$Salary))
```

```{r}
Hitters$Salary = log(Hitters$Salary)
```

### <span class="negrilla">(b)</span> {.letra}
Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

```{r}
train = 1:200
Hitters.train = Hitters[train, ]
Hitters.test = Hitters[-train, ]
```

### <span class="negrilla">(c)</span> {.letra}
Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter ?. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.


```{r, warning=FALSE, message=FALSE}
library(gbm)
```

```{r, warning=FALSE, message=FALSE}
set.seed(103)
learning.rate = 10 ^ seq(-10, -0.1, by = 0.1)
#plot(seq(-10, -0.1, by = 0.1),learning.rate, type = "l")
learning.rate.length = length(learning.rate)
train.errors = rep(NA, learning.rate.length)
test.errors = rep(NA, learning.rate.length)
for (i in 1:learning.rate.length) {
    hitters.boosting = gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", 
                            n.trees = 1000, shrinkage = learning.rate[i])
    
    train.pred = predict(hitters.boosting, Hitters.train, n.trees = 1000)
    test.pred = predict(hitters.boosting, Hitters.test, n.trees = 1000)
    
    train.errors[i] = mean((Hitters.train$Salary - train.pred)^2)
    test.errors[i] = mean((Hitters.test$Salary - test.pred)^2)
}
plot(learning.rate, train.errors, type = "b", xlab = "Tasa de aprendizaje", ylab = "MSE de entrenamiento", 
    col = "chocolate3", pch = 20)
```

```{r}
min(train.errors)
which.min(train.errors)
learning.rate[which.min(train.errors)]
```

### <span class="negrilla">(d)</span> {.letra}
Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.

```{r}
plot(learning.rate, test.errors, type = "b", xlab = "Tasa de aprendizaje", ylab = "MSE de prueba", 
    col = "cyan4", pch = 20)
```

```{r}
min.test.error = round(min(test.errors),5)
paste("EL minimo error de prueba es: ", min.test.error)
min.learning.rate = round(learning.rate[which.min(test.errors)],5)
paste("La tasa de aprendizaje que da un minimo error de prueba es de :", min.learning.rate)
```

### <span class="negrilla">(e)</span> {.letra}
Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.

```{r, warning=FALSE, message=FALSE}
library(glmnet)
#Usando regresion lineal multiple
lm.fit = lm(Salary ~ ., data = Hitters.train)
lm.pred = predict(lm.fit, Hitters.test)
mse = round(mean((Hitters.test$Salary - lm.pred)^2),5)
paste("el MSE usando un modelo de regresion lineal es de: ", mse)
#Usando Ridge Regression y Lasso
set.seed(134)
x = model.matrix(Salary ~ ., data = Hitters.train)
y = Hitters.train$Salary
x.test = model.matrix(Salary ~ ., data = Hitters.test)
#Ridge Regression
ridge.fit = glmnet(x, y, alpha = 0)
ridge.pred = predict(ridge.fit, s = 0.01, newx = x.test)
mse = round(mean((Hitters.test$Salary - ridge.pred)^2),5)
paste("el MSE usando un modelo Ridge Regression es de: ", mse)
#Lasso
lasso.fit = glmnet(x, y, alpha = 1)
lasso.pred = predict(lasso.fit, s = 0.01, newx = x.test)
mse = round(mean((Hitters.test$Salary - lasso.pred)^2),5)
paste("el MSE usando un modelo Lasso es de: ", mse)
```

<span class="negrita_verde">Se observa como el MSE es mucho mayor para modelos de Regresion Lineal, Ridge Regression y Lasso en comparacion con el modelo de Boosting.
</span>

### <span class="negrilla">(f)</span> {.letra}
Which variables appear to be the most important predictors in the boosted model?

```{r}
mejor.boost = gbm(Salary ~ ., data = Hitters.train, distribution = "gaussian", 
                  n.trees = 1000, shrinkage = learning.rate[which.min(test.errors)])
summary(mejor.boost)
```

<span class="negrita_verde">Las 5 variables de mayor importancia en el modelo son según su importancia: ***CAtBat***, ***CWalks***, ***CHits***, ***PutOuts***, ***Years***.
</span>

### <span class="negrilla">(g)</span> {.letra}
Now apply bagging to the training set. What is the test set MSE for this approach?

```{r, warning=FALSE, message=FALSE}
library(randomForest)
set.seed(21)
rf.hitters = randomForest(Salary ~ ., data = Hitters.train, ntree = 500, mtry = 19)
rf.pred = predict(rf.hitters, Hitters.test)
error = round(mean((Hitters.test$Salary - rf.pred)^2),5)
paste("EL MSE de prueba para el modelo bagging es de: ", error)
```


## <span class="titulo_ejercicio">Ejercicio 11</span> {.letra}

This question uses the Caravan data set.

### <span class="negrilla">(a)</span> {.letra}
Create a training set consisting of the first 1,000 observations, and a test set consisting of the remaining observations.

```{r, warning=FALSE}
library(ISLR)
library(gbm)
library(class)
#str(Caravan)
#rbind(head(Caravan,5),tail(Caravan,5))
train = 1:1000
Caravan$Purchase = ifelse(Caravan$Purchase == "Yes", 1, 0)
Caravan.train = Caravan[train, ]
Caravan.test = Caravan[-train, ]
```

### <span class="negrilla">(b)</span> {.letra}
Fit a boosting model to the training set with Purchase as the response and the other variables as predictors. Use 1,000 trees, and a shrinkage value of 0.01. Which predictors appear to be the most important?


```{r, warning=FALSE}
set.seed(400)
boost.caravan = gbm(Purchase ~ ., data = Caravan.train, n.trees = 1000, shrinkage = 0.01, 
                    distribution = "bernoulli")
summary(boost.caravan)
```

<span class="negrita_verde">Ordenadas segun su importancia las 5 variables mas importantes del modelo son respectivamente: **PPERSAUT**, **MKOOPKLA**, **MBERMIDD**, **MOPLHOOG**, **PBRAND**.
</span>

### <span class="negrilla">(c)</span> {.letra}
Use the boosting model to predict the response on the test data. Predict that a person will make a purchase if the estimated probability of purchase is greater than 20 %. Form a confusion matrix. What fraction of the people predicted to make a purchase do in fact make one? How does this compare with the results obtained from applying KNN or logistic regression to this data set?

```{r}
boost.prob = predict(boost.caravan, Caravan.test, n.trees = 1000, type = "response")
boost.pred = ifelse(boost.prob > 0.2, 1, 0)
matrix.conf = table(Caravan.test$Purchase, boost.pred)
verdadero.compr = round(matrix.conf[2,2]/sum(matrix.conf[,2]),2)
paste("La fraccion de personas que el modelo predijo como compradores y que efectuaron una compra es de:", verdadero.compr)
```

```{r, warning=FALSE}
#Usando Regresión Logistica.
lm.caravan = glm(Purchase ~ ., data = Caravan.train, family = binomial)
lm.prob = predict(lm.caravan, Caravan.test, type = "response")
lm.pred = ifelse(lm.prob > 0.2, 1, 0)
matrix.conf = table(Valor_real = Caravan.test$Purchase, Prediccion = lm.pred)
verdadero.compr = round(matrix.conf[2,2]/sum(matrix.conf[,2]),2)
matrix.conf
verdadero.compr
paste("La fraccion usando Regresión Logistica es:", verdadero.compr)
```

```{r, warning=FALSE}
#Usando KNN.
nros.k = 1:10
v.fracc = rep(NA,length(nros.k))
for(i in 1:length(nros.k)){
  Knn.predicc = knn(Caravan.train[,1:85], Caravan.test[,1:85], Caravan.train$Purchase, k = i, prob = TRUE )
  matrix.conf = table(Caravan.test$Purchase, Knn.predicc)
  verdadero.compr = round(matrix.conf[2,2]/sum(matrix.conf[,2]),4)
  v.fracc[i] = verdadero.compr
  Knn.predicc = NULL
}
paste("K =", nros.k, "fr =", v.fracc)
```

<span class="negrita_verde">Tanto para la regresión logistica como para KNN con diferentes valores para K la proporcion de verdaderos compradores es menor que la que se obtiene usando el metodo de boosting la cual es del **23%**. Aunque con un valor de K = 9 para KNN se acercan bastante.
</span>

## <span class="titulo_ejercicio">Ejercicio 12</span> {.letra}

Apply boosting, bagging, and random forests to a data set of your choice. Be sure to fit the models on a training set and to evaluate their performance on a test set. How accurate are the results compared to simple methods like linear or logistic regression? Which of these approaches yields the best performance?

<span class="negrita_verde">Para este ejercicio se hará uso del conjunto de datos College de la librería ISLR el cual registra datos en 18 variables de 777 universidades de los estados unidos. La primera de estas variables indica si la universidad es privada o no es privada. Se crearán entonces diferentes modelos de clasificación que permitirán predecir si una universidad es privada o no en función de las otras 17 variables y se evaluarán los rendimientos de estos modelos.
</span>

```{r, warning=FALSE}
library(ISLR)
library(gbm)
library(randomForest)
library(tidyverse)
#summary(College)
#str(OJ)
set.seed(100)
train = sample(nrow(College), 0.7* nrow(College))
train.college = College[train,]
test.college = College[-train,]
```

<span class="negrilla">MODELO DE REGRESIÓN LOGISTICA</span>

```{r}
#Probabilidades <= 0.5 corresponderan a universidades no privadas y > 0.5 a universidades privadas
glm.fit = glm(Private ~ ., data = train.college, family = "binomial")
glm.probs = predict(glm.fit, newdata = test.college, type = "response")
glm.pred = rep("No", length(glm.probs))
glm.pred[glm.probs > 0.5] = "Yes"
table(Prediccion = glm.pred, Valor_real = test.college$Private[])
porc.acierto = round(mean(glm.pred == test.college$Private)*100,2)
paste("Rendimiento usando Regresion Logistica =", porc.acierto, "%")
```

<span class="negrilla">MODELO USANDO BOOSTING</span>

```{r}
#Se crea una nueva variable binaria en función de la variable Private. Para poder usar la distribucion bernoulli en el parametro distribution
College$Private01 = ifelse(College$Private == "Yes", 1, 0)
college.boost = gbm(Private01 ~ ., data = College[train,-1], distribution = "bernoulli", n.trees = 5000) 
college.probs = predict(college.boost, newdata = College[-train,-1], n.trees = 5000)
college.predic = rep(0, length(college.probs))
college.predic[college.probs > 0.5] = 1
table(Prediccion = college.predic, Valor_real = College$Private01[-train])
porc.acierto = round(mean(college.predic == College$Private01[-train])*100,2)
paste("Rendimiento usando Bossting =", porc.acierto, "%")
College = College[,-19]
```

<span class="negrilla">MODELO USANDO BAGGING</span>

```{r}
college.bagg = randomForest(Private ~ ., data = train.college, mtry = 17)
college.bagg.pred = predict(college.bagg, newdata = test.college)
table(Prediccion = college.bagg.pred, Valor_real = test.college$Private)
porc.acierto = round(mean(college.bagg.pred == test.college$Private)*100,2)
paste("Rendimiento usando Bossting =", porc.acierto, "%")
```

<span class="negrilla">MODELO USANDO RANDOM FOREST</span>

```{r}
college.rf = randomForest(Private ~ ., data = train.college, mtry = 10)
college.rf.pred = predict(college.rf, newdata = test.college)
table(Prediccion = college.rf.pred, test.college$Private)
porc.acierto = round(mean(college.rf.pred == test.college$Private)*100,2)
paste("Rendimiento usando Bossting =", porc.acierto, "%")
```

<span class="negrilla subrayar">Los modelos con rendimiento mas alto fuerón los de <span class="naranga">**Bagging**</span> y <span class="naranga">**RandomForest**</span>. Los dos tienen una precisión mayor que el de la Regresion Logistica y el del metodo de Boosting. 
</span>

# <span class="titulo_capitulo">Capítulo 9.7</span> {#cap9_7}

## <span class="titulo_ejercicio">Ejercicio 4</span> {.letra}
<span class="negrilla subrayar">Generate a simulated two-class data set with 100 observations and two features in which there is a visible but non-linear separation between the two classes. Show that in this setting, a support vector machine with a polynomial kernel (with degree greater than 1) or a radial kernel will outperform a support vector classifier on the training data. Which technique performs best on the test data? Make
plots and report training and test error rates in order to back up your assertions.</span>

```{r, warning=FALSE}
library(e1071)
#Se crean los datos X y Y aleatorios
set.seed(200)
x = rnorm(100)
y = 4* -x^2 + 2 + rnorm(100)
#60% serán de la clase +1 y 40% de la clase -1
clasepos = sample(100, 60)
y[clasepos] = y[clasepos] +4
y[-clasepos] = y[-clasepos] -1
z = rep(-1, 100)
z[clasepos] = 1

plot(y[clasepos],x[clasepos], pch=2, lwd=4, col="cornflowerblue", xlim=c(-15, 10), xlab="Y", ylab="X")
points(y[-clasepos], x[-clasepos], pch=8, lwd=4, col="coral1")
```

```{r, warning=FALSE, message=FALSE}
#Se crean las variables respuestas -1 y +1
set.seed(300)
datos = data.frame(x,y,z = as.factor(z))
#70% de los datos se usarán para entrenar y 30% para prueba
train = sample(100, 70)
datos.train = datos[train,]
datos.test = datos[-train,]

#Se verifica la distribución de las clases en los datos de entrenamiento y de prueba
table(datos.train[,3])
table(datos.test[,3])

prop.table(table(datos.train[,3]))
prop.table(table(datos.test[,3]))
```

<span class="negrilla">Error en el conjunto de entrenamiento</span>
<br>
<span class="negrilla">KERNEL LINEAL</span>

```{r}
svm.linear = svm(z~., data=datos.train, kernel="linear", cost=10)
plot(svm.linear, datos.train)
```

```{r}
prediccion = predict(svm.linear, datos.train)
table(prediccion , real = datos.train$z)
train.error = round(mean(prediccion != datos.train$z)*100,3)

paste("Para el Kernel Lineal, el error de entrenamiento es: ", train.error, "%")
```

<span class="negrilla">KERNEL POLINOMIAL</span>
```{r}
set.seed(500)
svm.poly = svm(z~., data=datos.train, kernel="polynomial", cost=10, degree=11)
plot(svm.poly, datos.train)
```

```{r}
prediccion = predict(svm.poly, datos.train)
table(prediccion , real = datos.train$z)
train.error = round(mean(prediccion != datos.train$z)*100,3)

paste("Para un Kernel Polinomial de grado 11, el error de entrenamiento es: ", train.error, "%")
```

<span class="negrilla">KERNEL RADIAL</span>
```{r}
set.seed(600)
svm.radial = svm(z~., data=datos.train, kernel="radial", gamma=1, cost=10)
plot(svm.radial, datos.train)
```

```{r}
prediccion = predict(svm.radial, datos.train)
table(prediccion , real = datos.train$z)
train.error = round(mean(prediccion != datos.train$z)*100,3)

paste("Para un Kernel Radial el error de entrenamiento es: ", train.error, "%")
```

<span class="negrilla">Error en el conjunto de validación</span>
<br>
<span class="negrilla">KERNEL LINEAL</span>
```{r}
plot(svm.linear, datos.test)
```

```{r}
prediccion = predict(svm.linear, datos.test)
table(prediccion , real = datos.test$z)
test.error = round(mean(prediccion != datos.test$z)*100,3)

paste("Para un Kernel Lineal el error de prueba es: ", test.error, "%")
```
<span class="negrilla">KERNEL POLINOMIAL</span>
```{r}
plot(svm.poly, datos.test)
```

```{r}
prediccion = predict(svm.poly, datos.test)
table(prediccion , real = datos.test$z)
test.error = round(mean(prediccion != datos.test$z)*100,3)

paste("Para un Kernel Polinomial el error de prueba es: ", test.error, "%")
```
<span class="negrilla">KERNEL RADIAL</span>
```{r}
plot(svm.radial, datos.test)
```

```{r}
prediccion = predict(svm.radial, datos.test)
table(prediccion , real = datos.test$z)
test.error = round(mean(prediccion != datos.test$z)*100,3)

paste("Para un Kernel Radial el error de prueba es: ", test.error, "%")
```

Se puede evidenciar que el mejor modelo de SVM para este conjunto de datos es que usa un <span class="naranja">kernel radial</span> debido a que tiene un error del **0%** en los datos de entrenamiento y un error de prueba de **3.3%**. Si se compara el modelo SVM con kernel lineal y el modelo SVM con kernel polinomia se ve que el que usa kernel lineal tiene una mejor aproximación. Esto puede ser causado al alto grado del polinomio usado, causando que no se ajustan bien los datos.

## <span class="titulo_ejercicio">Ejercicio 5</span> {.letra}

<span class = "negrilla subrayas">We have seen that we can fit an SVM with a non-linear kernel in order to perform classification using a non-linear decision boundary.We will now see that we can also obtain a non-linear decision boundary by performing logistic regression using non-linear transformations of the features.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class ="negrilla">Generate a data set with n = 500 and p = 2, such that the observations belong to two classes with a quadratic decision boundary between them. For instance, you can do this as follows:</span>
> x1=runif (500) -0.5
> x2=runif (500) -0.5
> y=1*( x1^2-x2^2 > 0)

```{r}
set.seed(700)
x1 = runif(500) - 0.5
x2 = runif(500) - 0.5
y = 1*(x1^2 - x2^2 > 0)
```


### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Plot the observations, colored according to their class labels. Your plot should display X1 on the x-axis, and X2 on the yaxis.</span>

```{r}
plot(x1[y == 1], x2[y == 1], col = "darkred", xlab = "X1", ylab = "X2", pch = 4)
points(x1[y == 0], x2[y == 0], col = "forestgreen", pch = 19)
```

### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Fit a logistic regression model to the data, using X1 and X2 as predictors.</span>

```{r}
datos = data.frame(x1 = x1, x2 = x2, y = y)
lg.fit = glm(y ~ ., data = datos, family = binomial)
summary(lg.fit)
```


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be linear.</span>

```{r}
lg.probs = predict(lg.fit, newdata = datos, type = "response")
lg.pred = ifelse(lg.probs > 0.5, 1, 0)
datos.pos = datos[lg.pred == 1, ]
datos.neg = datos[lg.pred == 0, ]
plot(datos.pos$x1, datos.pos$x2, col = "darkred", xlab = "X1", ylab = "X2", pch = 4)
points(datos.neg$x1, datos.neg$x2, col = "forestgreen", pch = 19)
```


Se puede ver una clara diferenciación entre las dos clases usando regresión logística, incluso llegabdo a existir un límite línea para el conjunto de datos.


### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Now fit a logistic regression model to the data using non-linear functions of X1 and X2 as predictors (e.g. X21 , X1×X2, log(X2), and so forth).</span>

```{r, warning=FALSE}
lg.nolineal.fit = glm(y ~ poly(x1, 3) + poly(x2, 4) + I(x1*x2) , data = datos, family = binomial)
summary(lg.nolineal.fit)
```


### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Apply this model to the training data in order to obtain a predicted class label for each training observation. Plot the observations, colored according to the predicted class labels. The decision boundary should be obviously non-linear. If it is not, then repeat (a)-(e) until you come up with an example in which the predicted class labels are obviously non-linear.</span>

```{r}
lg.nolineal.probs = predict(lg.nolineal.fit, datos, type = "response")
lg.nolineal.pred = ifelse(lg.nolineal.probs > 0.5, 1, 0)
datos.pos = datos[lg.nolineal.pred == 1, ]
datos.neg = datos[lg.nolineal.pred == 0, ]
plot(datos.pos$x1, datos.pos$x2, col = "darkred", xlab = "X1", ylab = "X2", pch = 4)
points(datos.neg$x1, datos.neg$x2, col = "forestgreen", pch = 19)
```

A diferencia del modelo anterior, si se usa regresión logística anterior ya no existe el "límite lineal", por lo cual se habla de un límite no lineal que haga una división completa de las dos clases


### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Fit a support vector classifier to the data with X1 and X2 as predictors. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.</span>

```{r, warning=FALSE}
library(e1071)
svm.fit = svm(as.factor(y) ~ ., data = datos, kernel = "linear", cost = 1)
svm.pred = predict(svm.fit, datos)
datos.pos = datos[svm.pred == 1, ]
datos.neg = datos[svm.pred == 0, ]
plot(datos.pos$x1, datos.pos$x2, col = "darkred", xlab = "X1", ylab = "X2", pch = 4)
points(datos.pos$x1, datos.pos$x2, col = "forestgreen", pch = 19)
```


Se puede evidenciar que el modelo SVM de núcleo lineal no tiene la capacidad de identificar los limites entre las clase, por lo cual no realiza de forma correcta las predicciones.


### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla">Fit a SVM using a non-linear kernel to the data. Obtain a class prediction for each training observation. Plot the observations, colored according to the predicted class labels.</span>

```{r}
svm.fit = svm(as.factor(y) ~ ., datos, gamma = 1)
svm.pred = predict(svm.fit, datos)
datos.pos = datos[svm.pred == 1, ]
datos.neg = datos[svm.pred == 0, ]
plot(datos.pos$x1, datos.pos$x2, col = "darkred", xlab = "X1", ylab = "X2", pch = 4)
points(datos.neg$x1, datos.neg$x2, col = "forestgreen", pch = 19)
```


En cambio, si se usa un modelo SVM con un núcleo no lineal se puede identificar claramente la no linealidad entre las clases, lo que ocasiona una buena clasificación delas diferentes clases.


### <span class="negrilla">(i)</span> {.letra}
<span class="negrilla">Comment on your results.</span><br>

Como la relación entre los datos no es lineal, la regresión logística lineal o un SVM con núcleo lineal no sirven de mucho para clasificar estos datos. En cambio, se se usa una regresión logísitca no lineal o un SVM con kernel radial permiten clasificar de buena manera estos datos, siendo más fácil usar la regresión logística no lineal ya que depende solamente de un buen ajuste para el parámetro gamma.

    
## <span class="titulo_ejercicio">Ejercicio 6</span> {.letra}

<span class="negrilla subrayar">At the end of Section 9.6.1, it is claimed that in the case of data that is just barely linearly separable, a support vector classifier with a small value of cost that misclassifies a couple of training observations may perform better on test data than one with a huge value of cost that does not misclassify any training observations. You will now investigate this claim.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Generate two-class data with p = 2 in such a way that the classes are just barely linearly separable.</span>

```{r, warning=FALSE}
library(e1071)
library(ggplot2)
library(dplyr)
```
```{r, warning=FALSE}
set.seed(2520)

x.pos = runif(450, 0, 90)
y.pos = runif(450, x.pos + 10, 100)
x.pos.ruido = runif(50, 20, 80)
y.pos.ruido = 5/4 * (x.pos.ruido - 10) + 0.1

x.neg = runif(450, 10, 100)
y.neg = runif(450, 0, x.neg - 10)
x.neg.ruido = runif(50, 20, 80)
y.neg.ruido = 5/4 * (x.neg.ruido - 10) - 0.1

x = c(x.pos, x.pos.ruido, x.neg, x.neg.ruido)
y = c(y.pos, y.pos.ruido, y.neg, y.neg.ruido)

positivos = seq(1, 500)
z = rep(-1, 1000)
z[positivos] = 1

datos = data.frame(X = x, Y = y, z = as.factor(z))

ggplot(data = datos, aes(x = X, y = Y, color = as.factor(z))) +
  geom_point(size = 2) +
  theme_bw() +
  theme(legend.position = "none")
```

Al generar el conjunto de datos se puede evidenciar que no pueden ser separados por una línea reca, debido a que cada recta que se genere va a tener algunas clasificaciones erróneas de alguna de las clases.


### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Compute the cross-validation error rates for support vector classifiers with a range of cost values. How many training errors are misclassified for each value of cost considered, and how does this relate to the cross-validation errors obtained?</span>

```{r, warning=FALSE}
set.seed(325)
model.tuning <- tune(svm, z ~ ., data = datos, 
               kernel = "linear", 
               ranges = list(cost = c(0.01, 0.1, 1, 5, 10, 50, 100, 500, 1000,5000)))

summary(model.tuning)
ggplot(data = model.tuning$performances, aes(x = cost, y = error)) +
  geom_line(color= "darkorchid") +
  geom_point(color="darkorchid4") +
  labs(title = "Error de clasificación vs parámetro C") +
  theme_bw()
```
```{r, warning=FALSE}
paste("El mejor modelo es el siguiente:")
model.tuning$best.model
```

A medida que se aumenta el parámetro C (costo), se va a disminuir el error de entrenamiento. Utilizando el valor máximo del costo (c = 5000), se evidencia un uso de 49 vectores de soporte.


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Generate an appropriate test data set, and compute the test errors corresponding to each of the values of cost considered. Which value of cost leads to the fewest test errors, and how does this compare to the values of cost that yield the fewest training errors and the fewest cross-validation errors?</span>

```{r}
set.seed(2321)
x.test = runif(1000, 0, 100)
positivos = sample(1000, 500)
y.test = rep(NA, 1000)
for (i in positivos) {
    y.test[i] = runif(1, x.test[i], 100)
}
for (i in setdiff(1:1000, positivos)) {
    y.test[i] = runif(1, 0, x.test[i])
}

z.test = rep(-1, 1000)
z.test[positivos] = 1
datos.test = data.frame(X = x.test, Y = y.test, z = as.factor(z.test))

ggplot(data = datos.test, aes(x = x.test, y = y.test, color = as.factor(z.test))) +
  geom_point(size = 2) +
  theme_bw() +
  theme(legend.position = "none")
```

Ahora si se ve una clara separacion lineal entre este conjunto de datos.


```{r}
costos = c(0.01, 0.1, 1, 5, 10, 50, 100, 500,1000)
mal.clasificadas = rep(NA, length(costos))
errores.test = rep(NA, length(costos))
for (i in 1:length(costos)) {
    svm.fit = svm(z ~ ., data = datos, kernel = "linear", cost = costos[i])
    svm.predict = predict(svm.fit, datos.test)
    mal.clasificadas[i] = sum(svm.predict !=datos.test$z)
    errores.test[i] = mean(svm.predict != datos.test$z)*100
}
resultado = data.frame(costo = costos, mal_clasificadas = mal.clasificadas, porcentaje_error = errores.test)

ggplot(data = resultado, aes(x = costo, y = mal.clasificadas)) +
  geom_line(color= "firebrick") +
  geom_point(color="firebrick") +
  labs(title = "Error de clasificación vs parámetro costo") +
  theme_bw()

resultado
```

Con los datos de prueba se ve que no es necesario un valor de c tan alto como en el caso anterior. usando C = 5 o C = 10 se da una correcta clasificación de los datos


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Discuss your results.</span>

Al utilizar un valor de c tan alto en los datos de entrenamiento, el modelo se ajusta más a los datos, por lo cual se crea un sobreentrenamiento (overfitting). En cambio, con los datos de prueba se presenta un caso en que usando un valor de c = 5, se presenta un margen que no deja pasar tantos puntos ruidosos, que clasifica bien los datos y que además no presenta sobreentrenamiento. Esto se puede ver en la siguiente gráfica del hiperplano:


```{r, warning=FALSE}
#PRIMERA FORMA DE GRAFICAR EL MODELO FORMA BASICA
svm.5 = svm(z ~ ., data = datos, kernel = "linear", cost = 5)
plot(svm.5, datos)

#SEGUNDA FORMA DE GRAFICAR EL MODELO
predicciones = predict(svm.5, datos.test)

# Se almacenan los puntos predichos para dar color a las regiones
color_regiones = data.frame(datos.test, y = predicciones)
beta = drop(t(svm.5$coefs) %*% as.matrix(datos[,c("X","Y")])[svm.5$index,])
beta0 = svm.5$rho

# Representación de las 2 regiones empleando los puntos y coloreándolos
ggplot() +
  geom_point(data = color_regiones, aes(x = Y, y = X, color = as.factor(y)),size = 4) +
  # Se añaden las observaciones iniciales
  geom_point(data = datos, aes(x = Y, y = X, color = as.factor(z)),size = 2) +
  # Se identifican aquellas observaciones que son vectores soporte del modelo
  geom_point(data = datos[svm.5$index, ],
             aes(x = Y, y = X, color = as.factor(z)),
             shape = 21, colour = "black",
             size = 10) +
  # Se añaden las rectas del hiperplano y los márgenes
  geom_abline(intercept = beta0/beta[2], slope = -beta[1]/beta[2]) +
  geom_abline(intercept = (beta0 - 1)/beta[2], slope = -beta[1]/beta[2],
              linetype = "dashed") +    
  geom_abline(intercept = (beta0 + 1)/beta[2], slope = -beta[1]/beta[2],
              linetype = "dashed") +
  theme_bw() +
  theme(legend.position = "top")
```


## <span class="titulo_ejercicio">Ejercicio 7</span> {.letra}

<span class="negrilla subrayar">In this problem, you will use support vector approaches in order to predict whether a given car gets high or low gas mileage based on the Auto data set.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Create a binary variable that takes on a 1 for cars with gas mileage above the median, and a 0 for cars with gas mileage below the median.</span>

```{r, warning=FALSE}
library(ISLR)
library(e1071)
mediana = median(Auto$mpg)
nivel.gasol = ifelse(Auto$mpg > mediana, 1, 0)
Auto$nivel.gasol = as.factor(nivel.gasol)
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Fit a support vector classifier to the data with various values of cost, in order to predict whether a car gets high or low gas mileage. Report the cross-validation errors associated with different values of this parameter. Comment on your results.</span>

```{r}
set.seed(2000)
tuning.lineal = tune(svm, nivel.gasol ~ ., data = Auto, kernel = "linear", 
              ranges = list(cost = c(0.001, 0.01,0.1, 1, 5, 10, 20, 30)),
              scale = TRUE)

summary(tuning.lineal)
ggplot(data = tuning.lineal$performances, aes(x = cost, y = error)) +
  geom_line(color= "cyan4") +
  geom_point(color="cyan4") +
  labs(title = "Error de clasificación vs parametro C") +
  theme_bw()
```

```{r, warning=FALSE}
paste("EL MEJOR MODELO ESTÁ CONFIGURADO DE LA SIGUIENTE FORMA:")
summary(tuning.lineal$best.model)
```

Si se toman valores de C entre 0.01 y 1, se puede notar que a medida que aumenta C, el error decrece hasta llegar a valer <span class ="naranja">0.01275841</span> justo en C = 1. Cuando se toman valores mayores a 1, el error empieza a crecer lentamente. Con C = 1 se crea un modelo con 56 vectores de soporte que se reparten entre 26 para una clase y 30 para la otra.



### <span class="negrilla">(c)</span> {.letra}
<span class ="negrilla">Now repeat (b), this time using SVMs with radial and polynomial basis kernels, with different values of gamma and degree and cost. Comment on your results.</span>

```{r}
set.seed(500)
tuning.poli = tune(svm, nivel.gasol ~ ., data = Auto, kernel = "polynomial", 
                    ranges = list(cost = c(0.1, 1, 5, 10, 15, 20),
                    degree = c(2, 3, 4)),
                    scale = TRUE)
summary(tuning.poli)

ggplot(data = tuning.poli$performances, aes(x = cost, y = error, col = as.factor(degree))) +
  geom_line() +
  geom_point() +
  labs(title = "Error de clasificación vs parametro c y grado polinomio") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_bw() + theme(legend.position = "bottom")
```

```{r, warning=FALSE}
paste("EL MEJOR MODELO ESTÁ CONFIGURADO DE LA SIGUIENTE FORMA:")
summary(tuning.poli$best.model)
```

Con un polinomio de grado dos y un costo de 20 se alcanza el error mínimo, que vale <span class ="naranja">0.4345513</span>. El modelo svm configurado de esta manera usa 384 vectores de soporte que se reparten equitativamente entre cada clase (192 para cada una). Además, se puede notar graficamente que si se aumenta el grado del polinomio, el error no va a disminuir sin importar que se modifique el parámetro costo.


```{r}
set.seed(600)
tuning.radial = tune(svm, nivel.gasol ~ ., data = Auto, kernel = "radial", 
                    ranges = list(cost = c(0.1, 1, 5, 10,20), 
                    gamma = c(0.01, 0.1, 1, 5, 10, 20,50)))
summary(tuning.radial)

ggplot(data = tuning.radial$performances, aes(x = cost, y = error, color = factor(gamma))) +
  geom_line() +
  geom_point() +
  labs(title = "Error de Clasificación vs parámetros C y gamma") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme(legend.position = "bottom")
```

```{r, warning=FALSE}
paste("EL MEJOR MODELO ESTÁ CONFIGURADO DE LA SIGUIENTE FORMA:")
summary(tuning.radial$best.model)
```

En el caso radial el error mínimo se alcanza cuando se toma un costo = 20 y un parámetro gamma de 0.01. Con estos parámetos de obtiene un error de <span class ="naranja">0.01525641</span>, con un modelo SVM de 72 vectores de soporte que se reparten de la siguiente forma: 35 para una clase y 37 para la otra. El modelo con kernel radial es el que menor error presenta entre los 3 tipos de modelos posibles.


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Make some plots to back up your assertions in (b) and (c).</span>

*Hint: In the lab, we used the plot() function for svm objects only in cases with p = 2. When p > 2, you can use the plot() function to create plots displaying pairs of variables at a time. Essentially, instead of typing*

> plot(svmfit , dat)

*where svmfit contains your fitted model and dat is a data frame containing your data, you can type*

> plot(svmfit , dat , x1∼x4)

*in order to plot just the first and fourth variables. However, you must replace x1 and x4 with the correct variable names. To find out more, type ?plot.svm.*

<span class="negrilla">GRAFICAS PARA RESPALDAR EL MEJOR MODELO SVM LINEAL</span>

```{r, echo=FALSE}
#mpg y displacement
datos = Auto[,c("mpg", "displacement", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = displacement, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.lineal$best.model$index, ],
           aes(x = mpg, y = displacement, color = nivel.gasol),
           shape = 21, colour = "green",
           size = 5) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray0', colour='gray0'))

#mpg y horsepower
datos = Auto[,c("mpg", "horsepower", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = horsepower, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.lineal$best.model$index, ],
           aes(x = mpg, y = horsepower, color = nivel.gasol),
           shape = 21, colour = "green",
           size = 5) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray0', colour='gray0'))

#mpg y year
datos = Auto[,c("mpg", "year", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = year, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.lineal$best.model$index, ],
           aes(x = mpg, y = year, color = nivel.gasol),
           shape = 21, colour = "green",
           size = 5) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray0', colour='gray0'))

#mpg y acceleration
datos = Auto[,c("mpg", "acceleration", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = acceleration, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.lineal$best.model$index, ],
           aes(x = mpg, y = acceleration, color = nivel.gasol),
           shape = 21, colour = "green",
           size = 5) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray0', colour='gray0'))
```

<span class="negrita">GRÁFICAS CON EL MEJOR MODELO SVM POLINOMIAL</span>

```{r, echo=FALSE}
#mpg y displacement
datos = Auto[,c("mpg", "displacement", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = displacement, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.poli$best.model$index, ],
           aes(x = mpg, y = displacement, color = nivel.gasol),
           shape = 21, colour = "yellow",
           size = 3) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray69', colour='gray69'))

#mpg y horsepower
datos = Auto[,c("mpg", "horsepower", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = horsepower, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.poli$best.model$index, ],
           aes(x = mpg, y = horsepower, color = nivel.gasol),
           shape = 21, colour = "yellow",
           size = 3) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray69', colour='gray69'))

#mpg y year
datos = Auto[,c("mpg", "year", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = year, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.poli$best.model$index, ],
           aes(x = mpg, y = year, color = nivel.gasol),
           shape = 21, colour = "yellow",
           size = 3) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray69', colour='gray69'))

#mpg y acceleration
datos = Auto[,c("mpg", "acceleration", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = acceleration, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.poli$best.model$index, ],
           aes(x = mpg, y = acceleration, color = nivel.gasol),
           shape = 21, colour = "yellow",
           size = 3) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='gray69', colour='gray69'))
```

<span class="negrita">GRÁFICAS CON EL MEJOR MODELO SVM RADIAL</span>

```{r, echo=FALSE}
#mpg y displacement
datos = Auto[,c("mpg", "displacement", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = displacement, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.radial$best.model$index, ],
           aes(x = mpg, y = displacement, color = nivel.gasol),
           shape = 21, colour = "hotpink",
           size = 4) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='ivory', colour='ivory'))

#mpg y horsepower
datos = Auto[,c("mpg", "horsepower", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = horsepower, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.radial$best.model$index, ],
           aes(x = mpg, y = horsepower, color = nivel.gasol),
           shape = 21, colour = "hotpink",
           size = 4) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='ivory', colour='ivory'))

#mpg y year
datos = Auto[,c("mpg", "year", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = year, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.radial$best.model$index, ],
           aes(x = mpg, y = year, color = nivel.gasol),
           shape = 21, colour = "hotpink",
           size = 4) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='ivory', colour='ivory'))

#mpg y acceleration
datos = Auto[,c("mpg", "acceleration", "nivel.gasol")]
ggplot() +
geom_point(data = datos, aes(x = mpg, y = acceleration, color = nivel.gasol), size = 3) +
geom_point(data = datos[tuning.radial$best.model$index, ],
           aes(x = mpg, y = acceleration, color = nivel.gasol),
           shape = 21, colour = "hotpink",
           size = 4) +
theme(plot.title = element_text(hjust = 0.5)) +
theme_bw() + theme(legend.position = "top") +
theme(panel.background = element_rect(fill='ivory', colour='ivory'))
```

## <span class="titulo_ejercicio">Ejercicio 8</span> {.letra}

<span class ="negrilla subrayar">This problem involves the OJ data set which is part of the ISLR package.</span>

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.</span>

```{r, warning=FALSE}
library(ISLR)
library(e1071)
library(ggplot2)
set.seed(700)
train = sample(nrow(OJ), 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Fit a support vector classifier to the training data using
cost=0.01, with Purchase as the response and the other variables
as predictors. Use the summary() function to produce summary
statistics, and describe the results obtained.</span>

```{r}
svm.lineal = svm(Purchase ~ ., kernel = "linear", data = OJ.train, cost = 0.01)
summary(svm.lineal)
```

De las 800 observaciones, el SVM lineal usa 444 las cuales reparte de una manera bastante equitativa (223 para la clase CH y 221 para la clase MM).


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">What are the training and test error rates?</span>

```{r}
predic.train = predict(svm.lineal, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("El error de entrenamiento es: ", error.train)
```


```{r}
predic.test = predict(svm.lineal, OJ.test)
table(predicho = predic.test, OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba es: ", error.test)
```

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Use the tune() function to select an optimal cost. Consider values
in the range 0.01 to 10.</span>

```{r}
set.seed(200)
model.tuning = tune(svm, Purchase ~ ., data = OJ.train, kernel = "linear", 
                    ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(model.tuning)

ggplot(data = model.tuning$performances, aes(x = cost, y = error)) +
  geom_line(color = "steelblue1") +
  geom_point(color = "red") +
  labs(title = "Error de clasificación vs costo") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_bw() + theme(legend.position = "none")
```

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Compute the training and test error rates using this new value
for cost.</span>

Las caracterísiticas del modelo encontrado son:


```{r, warning=FALSE}
model.tuning$best.model
```

```{r}
mejor.svm.lineal = svm(Purchase ~ ., kernel = "linear", data = OJ.train, 
                       cost =model.tuning$best.parameters$cost)
predic.train = predict(mejor.svm.lineal, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("El error de entrenamiento con el mejor svm lineal es de: ", error.train)
```
```{r}
predic.test = predict(mejor.svm.lineal, OJ.test)
table(predicho = predic.test, real = OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba con el mejor svm lineal es de: ", error.test)
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Repeat parts (b) through (e) using a support vector machine
with a radial kernel. Use the default value for gamma.</span>

```{r}
set.seed(600)
svm.radial = svm(Purchase ~ ., data = OJ.train, kernel = "radial")
summary(svm.radial)
```

<span class ="naranja">SVM con kernel radial: </span> 368 observaciones como vectores de soporte, parte a la mitad (184) para cada clase.


```{r}
predic.train = predict(svm.radial, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("El error de entrenamiento es: ", error.train)
```

```{r}
predic.test = predict(svm.radial, OJ.test)
table(predicho = predic.test, real = OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba es: ", error.test)
```

```{r}
set.seed(400)
tuning.radial = tune(svm, Purchase ~ ., data = OJ.train, kernel = "radial",
                     ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tuning.radial)

ggplot(data = tuning.radial$performances, aes(x = cost, y = error)) +
  geom_line(color = "steelblue1") +
  geom_point(color = "green") +
  labs(title = "Error de clasificación vs costo") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_bw() + theme(legend.position = "none")
```

```{r}
mejor.svm.radial = svm(Purchase ~ ., data = OJ.train, kernel = "radial", 
                       cost = tuning.radial$best.parameters$cost)

predic.train = predict(mejor.svm.radial, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("EL error de entrenamiento con el mejor svm radial es de: ", error.train)
```

```{r}
predic.test = predict(mejor.svm.radial, OJ.test)
table(predicho = predic.test, real = OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba con el mejor svm radial es de: ", error.test)
```

### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Repeat parts (b) through (e) using a support vector machine
with a polynomial kernel. Set degree=2.</span>

```{r}
set.seed(1100)
svm.poly = svm(Purchase ~ ., data = OJ.train, kernel = "poly", degree = 2)
summary(svm.poly)
```

<span class ="naranja">SVM polinomial con grado 2: </span> 449 vectores de soporte distribuidos en 228 para la clase CH y 221 para la clase MM.


```{r}
predic.train = predict(svm.poly, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("El error de entrenamiento es: ", error.train)
```

```{r}
predic.test = predict(svm.poly, OJ.test)
table(predicho = predic.test, real = OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba es: ", error.test)
```


```{r}
set.seed(100)
tuning.poli = tune(svm, Purchase ~ ., data = OJ.train, kernel = "poly", degree = 2, 
    ranges = list(cost = 10^seq(-2, 1, by = 0.25)))
summary(tuning.poli)

ggplot(data = tuning.poli$performances, aes(x = cost, y = error)) +
  geom_line(color = "tan3") +
  geom_point(color = "seagreen") +
  labs(title = "Error de clasificación vs costo") +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_bw() + theme(legend.position = "none")
```

```{r}
mejor.svm.poly = svm(Purchase ~ ., data = OJ.train, kernel = "poly", degree = 2, 
                     cost = tuning.poli$best.parameters$cost)

predic.train = predict(mejor.svm.poly, OJ.train)
table(predicho = predic.train, real = OJ.train$Purchase)
error.train = round(mean(predic.train != OJ.train$Purchase),4)
paste("El error de entrenamiento con el mejor svm lineal es de: ", error.train)
```

```{r}
predic.test = predict(mejor.svm.poly, OJ.test)
table(predicho = predic.test, real = OJ.test$Purchase)
error.test = round(mean(predic.test != OJ.test$Purchase),4)
paste("El error de prueba con el mejor svm lineal es de: ", error.test)
```

### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla">Overall, which approach seems to give the best results on this
data?</span>

Con respecto a los datos de entrenamiento, quién presenta mejor rendimiento es el modelo SVM polinomial. Sin embargo, con respecto a los datos de prueba, el SVM lineal es el mejor modelo.