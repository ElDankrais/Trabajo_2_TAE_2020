---
title: "TAE - Accidentalidad en Medellín - Reporte Técnico"
author: "Jaime Andres Molina Correa <br> Valentina García Velasquez <br> Felipe Villarreal Piedrahita <br> Ricardo Penaloza Velasquez <br> Daniel Chanci Restrepo"
output:
  html_document:
    theme: cosmo
    highlight: kate
    css: format.css
    df_print: paged
    code_download: yes
    toc: true
    number_sections: true
    toc_float:
      collapsed: false
      smooth_scroll: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(Encoding="UTF-8")
 
library("ggplot2")                     # Load ggplot2 package
library("GGally")                      # Load GGally package
if(!require(pacman)){install.packages("pacman"); library(pacman)}
pacman::p_load("tidyverse", "knitr", "leaps")
source("functions.R", local = knitr::knit_global())
knitr::opts_chunk$set(echo = TRUE, comment = NA, tidy.opts = list(width.cutoff = 60), tidy = T)
```
#### <span>Universidad Nacional de Colombia sede Medellín</span> {.place}

#### Resumen {.letra}
En este trabajo se abarcan los ejercicios aplicados del texto guía "An Introduction to Statistical Learning with Applications in R" de la materia técnicas de aprendizaje estadístico impartida en la Universidad Nacional de Colombia sede Medellín.

# <span class="titulo_capitulo ">Capítulo 4.7 (Classification)</span> {#cap4_7}


## <span class="titulo_ejercicio">Ejercicio 10</span> {.letra}

<span class="negrilla subrayar">This question should be anwsered using the <span class="naranja">Weekly</span> data set, which is part of the <span class="naranja">ISLR</span> package. This data is similar in nature to the <span class="naranja">Smarket</span> data from this chapter’s lab, except that it contains 1, 089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010. </span> 

```{r, warning=FALSE, message=FALSE}
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
```
### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Produce some numerical and graphical summaries of the <span class="naranja">Weekly</span> data. Do there appear to be any patterns?</span>

```{r, warning=FALSE, message=FALSE}
library(ISLR)
data(Weekly)
attach(Weekly)
rbind(head(Weekly,10))
summary(Weekly)
#glimpse(Weekly)
#Se crea la matriz de correlacion
Weekly.corr <- round(cor(Weekly[,-9]),2)

Weekly2 <- subset( Weekly, select = -Direction )
pairs(Weekly2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Weekly2))
#ggpairs(Weekly2) 


```

Gracias a la gráfica anterior, podemos comprobar que la única gran correlación se da entre las variables año y volúmen. En realidad, ninguna de las demás varaibles logra tener una correlación mayor o igual al 10%.

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Use the full data set to perform a logistic regression with <span class="naranja">Direction</span> as the response and the five lag variables plus <span class="naranja">Volume</span> as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?</span>
```{r}
logist.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(logist.fit)
```

Debido a lo anterior, Lag2 es la única vairable que tiene un alto valor de significancia, ya que su valor p (0.0296) es menor a 0.05 y su β2 = 0.05844. Este valor es positivo, entonces muestra que si el valor fue positivo en el mercado hace 2 semanas, es más probable que lo sea en el día actual.


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.</span>

```{r}
logist.probab <- predict(logist.fit, type = "response")
logist.test <- rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"
matriz.confusion <- table(logist.test, Direction)
matriz.confusion
mosaic(matriz.confusion, shade = T, colorize = T, 
       gp = gpar(fill = matrix(c("yellow", "red", "red", "yellow"), 2, 2)))
```

Los calculos anteriores mostraron que la precisión de predicciones correctas es del 56.1% ((54 + 557) / (54 + 557 + 48 + 430)). También se logra evidenciar la gran diferencia que se da las semanas en las que el mercado sube y baja (cuando baja, la precisión es solo de 54 / (430 + 54) = 11.2%, mientras que cuando el mercado sube 557 / (557 + 48) = 92.1%).


### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Now fit the logistic regression model using a training data period from 1990 to 2008, with <span class="naranja">Lag2</span> as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).</span>

```{r}
data.train = (Year < 2009)
data.0910 = Weekly[!data.train, ]

logist.fit = glm(Direction ~ Lag2, data = Weekly, family = binomial, subset = data.train)

logist.probab = predict(logist.fit, data.0910, type = "response")
logist.test = rep("Down", length(logist.probab))
logist.test[logist.probab > 0.5] = "Up"

Direction.0910 = Direction[!data.train]
table(logist.test, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(logist.test == Direction.0910)*100, "%")
```

Luego de ajustar el modelo con los datos en el intervalo de tiempo dado y con la variable solicitada, se llegó a un 62.5% de predicciones correctas, lo que ocasiona un <span class="negrilla">test error rate</span> de 37.5%. En este mismo orden de ideas, se obtiene que el procetaje de observaciones correctas para las semanas con un valor en alza es de 91,8% (56/(56+5)) mientras que las semanas con un valor de mercado a la baja es de 20,93% (9/(9+34)).

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Repeat (d) using LDA.</span>

```{r}
lda.fit = lda(Direction ~ Lag2, data = Weekly, subset = data.train)
lda.test = predict(lda.fit, data.0910)
table(lda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", mean(lda.test$class == Direction.0910)*100, "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Repeat (d) using QDA.</span>

```{r}
qda.fit = qda(Direction ~ Lag2, data = Weekly, subset = data.train)
qda.test = predict(qda.fit, data.0910)
table(qda.test$class, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(qda.test$class == Direction.0910)*100,1), "%")
```

### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Repeat (d) using KNN with K = 1.</span>

```{r}
train.X = as.matrix(Lag2[data.train])
test.X = as.matrix(Lag2[!data.train])
train.Direction = Direction[data.train]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)
table(knn.pred, Direction.0910)
```

```{r, echo=FALSE}
paste("PORCENTAJE DE PREDICCIONES CORRECTAS =", round(mean(knn.pred == Direction.0910)*100,1), "%")
```


### <span class="negrilla">(h)</span> {.letra}
<span class="negrilla">Which of these methods appears to provide the best results on this data?</span>


Aparentemente, los métodos de Regresion Logistica (62.5%) y LDA (62.5%) tienen la mayor precisión con igual resultado, en contraste con QDA (58.7%) y KNN (50%).

--------------------------------------------


## <span class="titulo_ejercicio">Ejercicio 11</span> {.letra}

<span class="negrilla subrayar">In this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the <span class="naranja">Auto</span> data set.</span>
```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(corrplot)
library(psych)
library(MASS)
library(class)
library(Hmisc)
library(PerformanceAnalytics)
library(ggplot2)
library(vcd)
library(GGally)
library(caret)
library(corrplot)
```

### <span class="negrilla">(a)</span> {.letra}
<span class="negrilla">Create a binary variable, <span class="naranja">mpg01</span>, that contains a 1 if <span class="naranja">mpg</span> contains a value above its median, and a 0 if <span class="naranja">mpg</span> contains a value below its median. You can compute the median using the <span class="naranja">median()</span> function. Note you may find it helpful to use the <span class="naranja">data.frame()</span> function to create a single data set containing both <span class="naranja">mpg01</span> and the other <span class="naranja">Auto</span> variables.</span>

```{r, warning=FALSE, message=FALSE}
rbind(head(Auto,10))
summary(Auto)
```

```{r, warning=FALSE, message=FALSE}
attach(Auto)
mpg01 = rep(0, length(mpg))
mpg01[mpg > median(mpg)] = 1
Auto = data.frame(Auto, mpg01)
rbind(head(Auto,5), tail(Auto,5))
str(Auto)
```

### <span class="negrilla">(b)</span> {.letra}
<span class="negrilla">Explore the data graphically in order to investigate the association between <span class="naranja">mpg01</span> and the other features. Which of the other features seem most likely to be useful in predicting <span class="naranja">mpg01</span>? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.</span>

```{r, warning=FALSE, message=FALSE}
Auto2 <- subset( Auto, select = -name )
pairs(Auto2, lower.panel = myPanel.cor, upper.panel = panel.smooth, diag.panel = myPanel.box, labels = names(Auto2))


corrplot.mixed(cor(Auto2), order="hclust", tl.col="black")
```

La variable mpg01 posee cierta correlación con todas las variables. Como era de esperarse, la correlación mas positiva la tiene con mpg (84%), pero, de manera negativa, tiene una correlación considerable con displacement (-75%), cylinders (-76%) y weight (-76%).


### <span class="negrilla">(c)</span> {.letra}
<span class="negrilla">Split the data into a training set and a test set.</span>

```{r, warning=FALSE, message=FALSE}
set.seed(99)
partition <- createDataPartition(y = Auto$mpg01, p = 0.7, list = FALSE, times = 1)
Auto.train = Auto[partition, ]
Auto.test = Auto[-partition, ]
mpg01.test = mpg01[-partition]
```

### <span class="negrilla">(d)</span> {.letra}
<span class="negrilla">Perform LDA on the training data in order to predict mpg01 using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r}
lda.fit = lda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
lda.pred = predict(lda.fit, Auto.test)
prop.correct = mean(lda.pred$class == mpg01.test)
```

```{r, echo=FALSE}
paste("Error LDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(e)</span> {.letra}
<span class="negrilla">Perform QDA on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What is the test error of the model obtained?</span>

```{r, warning=FALSE, message=FALSE}
# QDA
qda.fit = qda(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train)
qda.pred = predict(qda.fit, Auto.test)
prop.correct = mean(qda.pred$class == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error QDA =", round((1 - prop.correct)*100,1), "%")
```

### <span class="negrilla">(f)</span> {.letra}
<span class="negrilla">Perform logistic regression on the training data in order to predict <span class="naranja">mpg01</span> using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?</span>

```{r}
#REGRESION LOGISTICA
logist.fit = glm(mpg01 ~ cylinders + weight + displacement + horsepower, data = Auto.train, 
    family = binomial)
logist.probs = predict(logist.fit, Auto.test, type = "response")
logist.pred = rep(0, length(logist.probs))
logist.pred[logist.probs > 0.5] = 1
prop.correct = mean(logist.pred == mpg01.test)
#round(prop.correct,3)
```
```{r, echo=FALSE}
paste("Error REGRESION LOGISTICA =", round((1 - prop.correct)*100,1), "%")
```


### <span class="negrilla">(g)</span> {.letra}
<span class="negrilla">Perform KNN on the training data, with several values of K, in order to predict <span class="naranja">mpg01</span>. Use only the variables that seemed most associated with <span class="naranja">mpg01</span> in (b). What test errors do you obtain? Which value of K seems to perform the best on this data set?</span>

```{r}
train.X = cbind(cylinders, weight, displacement, horsepower)[partition, ]
test.X = cbind(cylinders, weight, displacement, horsepower)[-partition, ]
train.mpg01 = mpg01[partition]
set.seed(13)

knn.pred = knn(train.X, test.X, train.mpg01, k = 1)
prop.correct = mean(knn.pred == mpg01.test)

```
```{r, echo=FALSE}
paste("Error con KNN (K=1) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 5)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=5) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 25)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=25) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 50)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=50) =", round((1 - prop.correct)*100,1), "%")
```

```{r}
knn.pred = knn(train.X, test.X, train.mpg01, k = 100)
prop.correct = mean(knn.pred == mpg01.test)
```
```{r, echo=FALSE}
paste("Error con KNN (K=100) =", round((1 - prop.correct)*100,1), "%")
```

K=5 fue el valor con el menor error e todos, equivalente a 8.6%. 